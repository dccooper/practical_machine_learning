---
title: "Practical Machine Leanring - Final"
author: "David Cooper"
date: "April 1, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.  

# Preparation
The training data for this project are available here:  
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:  
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. 

```{r, echo=TRUE, message=FALSE}
library(caret) # Loading in the caret library

# Importing the data sets
set_train <- read.csv("data/pml-training.csv")
set_test <- read.csv("data/pml-testing.csv")

# Splitting the training set for cross validation
set.seed(123)
train_split <- createDataPartition(y = set_train$classe, p = 0.7, list = FALSE)
set_train1 <- set_train[train_split, ]
set_train2 <- set_train[-train_split, ]
```
### Reducing the dataset
In order to make model building and prediction faster, we should reduce the dataset to the smallest possible size. We can do this by only accounting for the variables that we are interested in predicting. We can also exclude data that does not vary by much, and so would not be helpful in making any predictions.  
```{r, echo=TRUE}
# Removing variables that are predominately full of NA values
majority_NA <- sapply(set_train1, function(x) mean(is.na(x))) > 0.95
set_train1 <- set_train1[, majority_NA == FALSE]
set_train2 <- set_train2[, majority_NA == FALSE]

# Removing unneccesary variables for prediction:
# (X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp)
set_train1 <- set_train1[, -(1:5)]
set_train2 <- set_train2[, -(1:5)]

# Excluding variables with near zero variance
zero_var <- nearZeroVar(set_train1)
set_train1 <- set_train1[, -zero_var]
set_train2 <- set_train2[, -zero_var]
```
  
Removing these variables allows us to reduce the overall dataset from 160 variables to 54 - halving the amount of data our model has to take into account.

# Model Building
Let's begin with a more visual approach, the *Decision Tree Method* 
.
```{r, echo=TRUE, message=FALSE}
library(rattle) # Importing the Rattle library
library(rpart)

set.seed(123)
model_dt <- train(classe~., method = "rpart", data = set_train1)

# Printing the final model
print(model_dt$finalModel)

# Visualizing the final model with Rattle
fancyRpartPlot(model_dt$finalModel,cex=.5,under.cex=1,shadow.offset=0)

```
  
  
We will next try a model discussed frequently in the course, the *Random Forest* approach. 
```{r echo=TRUE, message=FALSE}
library(randomForest) # Loading in the Random Forest library

# Setting up, using a cross validation approach
control_fit <- trainControl(method = "cv", number = 2)

# Building the model
model_fit <- train(classe ~. , data = set_train1, method = "rf", trControl = control_fit)

model_fit$finalModel
```


# Model Evaluation
Now let's evaluate both of these models to see how well they perform in prediction using our cross validation dataset.

```{r echo=TRUE, message=FALSE}
# Testing the Decision Tree Model
set.seed(123)
predict_tree <- predict(model_dt, set_train2) 
cross_tree <- confusionMatrix(predict_tree, set_train2$classe)
cross_tree  
```
  
Looking at these results, we can see that the Decision Tree model we used only gave us a ~60% accurate prediction with a 50% in-sample error rate on our cross-validation set.  
   
Let's try looking at the Random Forest model to see if it is any better.  
  
```{r, echo=TRUE, message=FALSE}
# Testing the Random Forest model
predict_forest <- predict(model_fit, set_train2)
cross_forest <- confusionMatrix(predict_forest, set_train2$classe)
cross_forest
```
  
We can clearly see that - at over 99% accurate with around a 0.3% in-sample error rate - the *Random Forest* model significantly outperforms the *Decision Tree* model. 

### Testing for Out-of-Sample Error
So it appears from our accuracy and in-sample error rate (1-Kappa), we have pretty good model. However, that could be a sign that we have overfitted it. Let's look at the out-of-sample error rate just to check.
```{r echo=TRUE, message=FALSE}
# Calculate the out-of-sample error
OSE <- sum(predict_forest == set_train2$classe)/length(predict_forest)

# Set out-of-sample error rate as a percentage
OSE <- round(((1 - OSE)*100), digits = 2)
```

The out-of-sample error rate for this model is `r OSE`%, which is good because it shows we still have some wiggle room in our model. So after all this, it looks like the *Random Forest* Model **will** be the one we will use going forward.

# Model Retraining
Now that we have cross-validated our models and selected the best one, let's retrain the *Random Forest* model using the entire training set. We will start by re-running the data cleaning we did for the individual training sets on the entire original test dataset. 
```{r, echo=TRUE, message=FALSE}
# Removing variables that are predominately full of NA values
majority_NA <- sapply(set_train, function(x) mean(is.na(x))) > 0.95
set_train <- set_train[, majority_NA == FALSE]

# Removing unneccesary variables for prediction:
# (X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp)
set_train <- set_train[, -(1:5)]

# Excluding variables with near zero variance
zero_var <- nearZeroVar(set_train)
set_train <- set_train[, -zero_var]
```
  
Now let's refit the *Random Forest* model using the entire training data set.
```{r, echo=TRUE, message=FALSE}
# Building the final model
final_fit <- train(classe ~. , data = set_train, method = "rf", trControl = control_fit)
```
# Model Predictions
Let's apply our model to our test dataset. These final prediction values can be used to answer the Prediction Quiz section of the final project. 
```{r echo=TRUE,message=FALSE}
final_prediction <- predict(final_fit, set_test)
final_prediction
```


# References
Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. [Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements.](http://groupware.les.inf.puc-rio.br/work.jsf?p1=10335) Proceedings of 21st Brazilian Symposium on Artificial Intelligence. *Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science.* , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. 

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz4d0yFCAVA
